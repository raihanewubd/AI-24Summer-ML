{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raihanewubd/AI-24Summer-ML/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of Convolutional Neural Network (CNN)**"
      ],
      "metadata": {
        "id": "2bSFlffcgmGI"
      },
      "id": "2bSFlffcgmGI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MAVkg8-BBN3q",
      "metadata": {
        "id": "MAVkg8-BBN3q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2834ab4a",
      "metadata": {
        "id": "2834ab4a"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038533d3",
      "metadata": {
        "id": "038533d3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "import platform\n",
        "import psutil\n",
        "\n",
        "import random\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from PIL import ImageOps\n",
        "\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf72bea6",
      "metadata": {
        "id": "cf72bea6"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4079ea7",
      "metadata": {
        "id": "d4079ea7"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/CNN/dataset\"\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "for class_name in os.listdir(data_dir):\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    for image_name in os.listdir(class_dir):\n",
        "        file_paths.append(os.path.join(class_dir, image_name))\n",
        "        labels.append(class_name)\n",
        "\n",
        "df = pd.DataFrame({\"file_path\": file_paths, \"label\": labels})\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee7d9da9",
      "metadata": {
        "id": "ee7d9da9"
      },
      "outputs": [],
      "source": [
        "class_counts_train = df['label'].value_counts()\n",
        "\n",
        "for class_name, count in class_counts_train.items():\n",
        "    print(f\"Class: {class_name}, Count: {count}\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "ax = class_counts_train.plot(kind='bar')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Amount of data')\n",
        "plt.xticks(rotation=360)\n",
        "for i, count in enumerate(class_counts_train):\n",
        "    ax.text(i, count + 5, str(count), ha='center')\n",
        "plt.ylim(0, max(class_counts_train) * 1.2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = random.randint(1, len(df) - 1)\n",
        "random_row = df.iloc[random_index]\n",
        "\n",
        "file_path = random_row['file_path']\n",
        "label = random_row['label']\n",
        "\n",
        "image = Image.open(file_path)\n",
        "\n",
        "size = image.size\n",
        "channels = 'Grayscale' if image.mode == 'L' else 'RGB'\n",
        "plt.title(f\"Label: {label}\\nSize: {size}\\nChannels: {channels}\")\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_sf1e_3zXxmn"
      },
      "id": "_sf1e_3zXxmn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1c60ef68",
      "metadata": {
        "id": "1c60ef68"
      },
      "source": [
        "###  Split the Dataset into Training, Validation and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3b67e0",
      "metadata": {
        "id": "1a3b67e0"
      },
      "outputs": [],
      "source": [
        "train_dataframe, temp_dataframe = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "test_dataframe, valid_dataframe = train_test_split(temp_dataframe, test_size=0.5, stratify=temp_dataframe['label'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77693087",
      "metadata": {
        "id": "77693087"
      },
      "outputs": [],
      "source": [
        "print(\"Training Data: \", len(train_dataframe))\n",
        "print(\"Validation Data: \", len(valid_dataframe))\n",
        "print(\"Test Data: \", len(test_dataframe))\n",
        "print(\"-------------------------------------------\")\n",
        "print(\"Total amounts of data in the dataset: \", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8db9c0",
      "metadata": {
        "id": "2a8db9c0"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbecfbfc",
      "metadata": {
        "id": "cbecfbfc"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65dbd001",
      "metadata": {
        "id": "65dbd001"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa09cea1",
      "metadata": {
        "id": "fa09cea1"
      },
      "outputs": [],
      "source": [
        "save_path_checkpoints = \"/content/drive/MyDrive/Colab Notebooks/CNN\"\n",
        "os.makedirs(save_path_checkpoints, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2473bec6",
      "metadata": {
        "id": "2473bec6"
      },
      "source": [
        "### Dataset Class, Dataloader and Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74560017",
      "metadata": {
        "id": "74560017"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, normalize, is_lb=False):\n",
        "        self.dataframe = dataframe\n",
        "        self.normalize = normalize\n",
        "        self.is_lb = is_lb\n",
        "        self.label_map = {'angry': 0, 'happy': 1, 'sad': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.dataframe.iloc[index]['file_path']\n",
        "        image = PIL.Image.open(img_path)\n",
        "\n",
        "        if self.is_lb:\n",
        "            label = self.label_map[self.dataframe.iloc[index]['label']]\n",
        "            return self.normalize(image), label\n",
        "        else:\n",
        "            return self.normalize(image), self.normalize(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e72ed5a1",
      "metadata": {
        "id": "e72ed5a1"
      },
      "outputs": [],
      "source": [
        "batch = 8\n",
        "\n",
        "def data_transfrom():\n",
        "    normalize = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_dataset = ImageDataset (\n",
        "        train_dataframe,\n",
        "        normalize,\n",
        "        is_lb=True\n",
        "    )\n",
        "\n",
        "    valid_dataset = ImageDataset(\n",
        "        valid_dataframe,\n",
        "        normalize,\n",
        "        is_lb=True\n",
        "    )\n",
        "\n",
        "    test_dataset = ImageDataset(\n",
        "        test_dataframe,\n",
        "        normalize,\n",
        "        is_lb=True\n",
        "    )\n",
        "\n",
        "    dataloader_train_dataset = DataLoader(train_dataset, batch_size = batch, shuffle=True, num_workers=2)\n",
        "    dataloader_valid_dataset = DataLoader(valid_dataset, batch_size = batch, shuffle=False, num_workers=2)\n",
        "    dataloader_test_dataset = DataLoader(test_dataset, batch_size = batch, shuffle=False, num_workers=2)\n",
        "\n",
        "    return dataloader_train_dataset, dataloader_valid_dataset, dataloader_test_dataset\n",
        "\n",
        "dataloader_train_dataset, dataloader_valid_dataset, dataloader_test_dataset = data_transfrom()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21be3d22",
      "metadata": {
        "id": "21be3d22"
      },
      "source": [
        "### Design and Implementation of CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_model, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 3)  # 3 output classes\n",
        "\n",
        "        # Dropout layer (to prevent overfitting)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional block\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "\n",
        "        # Second convolutional block\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Third convolutional block\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        # Flatten the output for the fully connected layers\n",
        "        x = x.view(-1, 128 * 28 * 28)\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Wndr2JquTTqs"
      },
      "id": "Wndr2JquTTqs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_model()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Ewd6w76Shesg"
      },
      "id": "Ewd6w76Shesg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters)"
      ],
      "metadata": {
        "id": "Ok6CAQqvhYDE"
      },
      "id": "Ok6CAQqvhYDE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)"
      ],
      "metadata": {
        "id": "OtSgsYQQhpcY"
      },
      "id": "OtSgsYQQhpcY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a24a8be1",
      "metadata": {
        "id": "a24a8be1"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1lynG8bMd4dr",
      "metadata": {
        "id": "1lynG8bMd4dr"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "def train_model(model, criterion, optimizer, dataloader_train_dataset, dataloader_valid_dataset, num_epochs=10, early_stop_patience=3, save_path_checkpoints=\"checkpoints\"):\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    consecutive_no_improvement = 0\n",
        "    num_epochs_loss_greater = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        progress_bar = tqdm(enumerate(dataloader_train_dataset), total=len(dataloader_train_dataset))\n",
        "        for i, (inputs, labels) in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            progress_bar.set_postfix(loss=running_loss / total_train, acc=correct_train / total_train)\n",
        "        epoch_train_loss = running_loss / total_train\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "        train_loss_history.append(epoch_train_loss)\n",
        "        train_acc_history.append(epoch_train_acc)\n",
        "\n",
        "        print('Training Loss: {:.3f} Acc: {:.3f}'.format(epoch_train_loss, epoch_train_acc))\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader_valid_dataset:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_loss / total_val\n",
        "        epoch_val_acc = correct_val / total_val\n",
        "        val_loss_history.append(epoch_val_loss)\n",
        "        val_acc_history.append(epoch_val_acc)\n",
        "\n",
        "        print('Validation Loss: {:.3f} Acc: {:.3f}'.format(epoch_val_loss, epoch_val_acc))\n",
        "\n",
        "        # Check for early stopping\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            best_epoch = epoch + 1\n",
        "            filepath = f\"{save_path_checkpoints}/model.pt\"\n",
        "            checkpoint = {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"model_weight\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict()\n",
        "            }\n",
        "            torch.save(checkpoint, filepath)\n",
        "            print(f\"Best model saved at epoch {best_epoch} with validation accuracy: {best_val_acc:.3f}\")\n",
        "            consecutive_no_improvement = 0\n",
        "        else:\n",
        "            consecutive_no_improvement += 1\n",
        "\n",
        "        if epoch_val_loss > epoch_train_loss:\n",
        "            num_epochs_loss_greater += 1\n",
        "        else:\n",
        "            num_epochs_loss_greater = 0\n",
        "\n",
        "        if consecutive_no_improvement >= early_stop_patience or num_epochs_loss_greater >= early_stop_patience:\n",
        "            print(f\"Early stopping criteria met. No improvement in validation accuracy or validation loss for {early_stop_patience} consecutive epochs. Training stopped.\")\n",
        "            break\n",
        "\n",
        "    return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
        "\n",
        "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(model, criterion, optimizer, dataloader_train_dataset, dataloader_valid_dataset, save_path_checkpoints=save_path_checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bs1FUrKPDWWC",
      "metadata": {
        "id": "Bs1FUrKPDWWC"
      },
      "outputs": [],
      "source": [
        "def training_curves(train_loss_history, val_loss_history, train_acc_history, val_acc_history):\n",
        "    epochs = range(1, len(train_loss_history) + 1)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss_history, label='Training Loss')\n",
        "    plt.plot(epochs, val_loss_history, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_acc_history, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc_history, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "training_curves(train_loss_history, val_loss_history, train_acc_history, val_acc_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4eead2",
      "metadata": {
        "id": "8e4eead2"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7xeKVCHMqhJY",
      "metadata": {
        "id": "7xeKVCHMqhJY"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, criterion, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901810fc",
      "metadata": {
        "id": "901810fc"
      },
      "outputs": [],
      "source": [
        "best_checkpoint_path = f\"{save_path_checkpoints}/model.pt\"\n",
        "checkpoint = torch.load(best_checkpoint_path)\n",
        "best_epoch = checkpoint[\"epoch\"]\n",
        "model.load_state_dict(checkpoint[\"model_weight\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "\n",
        "best_val_loss, best_val_accuracy = evaluate_model(model, criterion, dataloader_valid_dataset)\n",
        "print(f\"Best model (from epoch {best_epoch}) - Validation Loss: {best_val_loss:.3f}, Validation Accuracy: {best_val_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86HEuGa9Np1U",
      "metadata": {
        "id": "86HEuGa9Np1U"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = evaluate_model(model, criterion, dataloader_test_dataset)\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3b5b94",
      "metadata": {
        "id": "4b3b5b94"
      },
      "outputs": [],
      "source": [
        "def class_accuracy(model, dataloader, num_classes):\n",
        "    class_correct = [0.0] * num_classes\n",
        "    class_total = [0.0] * num_classes\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct = (predicted == labels)\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += correct[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    class_accuracy = [class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in range(num_classes)]\n",
        "\n",
        "    return class_accuracy\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader_test_dataset:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "class_names = [str(i) for i in range(3)]\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "class_acc = class_accuracy(model, dataloader_test_dataset, 3)\n",
        "class_names = {0: \"angry\", 1: \"happy\", 2: \"sad\"}\n",
        "total_class = len(class_names)\n",
        "for i in range(total_class):\n",
        "    print(f\"{class_names[i]} Accuracy: {class_acc[i]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "eu4-fKLdiJ9m"
      },
      "id": "eu4-fKLdiJ9m"
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(class_names.values()), yticklabels=list(class_names.values()))\n",
        "\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tR47_QJ4iGRd"
      },
      "id": "tR47_QJ4iGRd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}